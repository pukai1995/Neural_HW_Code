{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3fc93d99-758c-4348-bb3e-8c777aa39728",
      "metadata": {
        "id": "3fc93d99-758c-4348-bb3e-8c777aa39728"
      },
      "source": [
        "Change our model to use a 5 × 5 kernel with kernel_size=5 passed to the nn.Conv2d constructor. <br>\n",
        "a What impact does this change have on the number of parameters in the model?<br>\n",
        "b Does the change improve or degrade overfitting?<br>\n",
        "c Read https://pytorch.org/docs/stable/nn.html#conv2d.<br>\n",
        "d Can you describe what kernel_size=(1,3) will do?<br>\n",
        "e How does the model behave with such a kernel?<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef56940c-c18c-4a06-b538-b2b0727a68d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef56940c-c18c-4a06-b538-b2b0727a68d0",
        "outputId": "c0620811-1916-4b2b-f84c-21c334dc11f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e6c98299a90>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da650d2f-60a5-4029-94b2-905866f7618a",
      "metadata": {
        "id": "da650d2f-60a5-4029-94b2-905866f7618a"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5b048c-0cb4-4b6e-8dc5-d3d0c7760423",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec5b048c-0cb4-4b6e-8dc5-d3d0c7760423",
        "outputId": "196d8932-0588-43cc-a92b-de1e0cd9c20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 60942676.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversioned/p1ch6/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "data_path = '../data-unversioned/p1ch6/'\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c34ffa-3664-475a-8f57-cca15cb0138a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5c34ffa-3664-475a-8f57-cca15cb0138a",
        "outputId": "89c26a6d-d1f1-4577-870d-ff32ffb694cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7cf5a6-df01-43eb-b317-54ce8cd6f01f",
      "metadata": {
        "id": "6d7cf5a6-df01-43eb-b317-54ce8cd6f01f"
      },
      "outputs": [],
      "source": [
        "label_map = {0: 0, 2: 1}\n",
        "class_names = ['airplane', 'bird']\n",
        "cifar2 = [(img, label_map[label])\n",
        "          for img, label in cifar10\n",
        "          if label in [0, 2]]\n",
        "cifar2_val = [(img, label_map[label])\n",
        "              for img, label in cifar10_val\n",
        "              if label in [0, 2]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c13b95-de9b-47b7-a016-f405cd0d6b53",
      "metadata": {
        "id": "71c13b95-de9b-47b7-a016-f405cd0d6b53"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula given for calculating the output size (one dimension) of a convolution is (W−F+2P)/S+1\n",
        "\n",
        "If you want to keep the output size same as the input size, you can equate (W−max(F,S)+2P)/S+1=W\n",
        "Solving this, when S=1\n",
        " gives:\n",
        "\n",
        "W−F+2P+1=W\n",
        "\n",
        "2P=F−1\n",
        "\n",
        "When S>1\n",
        ", you could solve the amount of padding needed to keep the output size same as the input size:\n",
        "\n",
        "2P=max(F,S)−S\n",
        "\n",
        "https://stats.stackexchange.com/questions/297678/how-to-calculate-optimal-zero-padding-for-convolutional-neural-networks"
      ],
      "metadata": {
        "id": "1MfTKAhylMkJ"
      },
      "id": "1MfTKAhylMkJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "089f677d-8037-4a47-81c2-d2c7a3bc1ffd",
      "metadata": {
        "id": "089f677d-8037-4a47-81c2-d2c7a3bc1ffd"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=5, padding=2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * 8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5187844-403e-4852-ab8b-23608f7ca2bb",
      "metadata": {
        "id": "f5187844-403e-4852-ab8b-23608f7ca2bb"
      },
      "source": [
        "Number of Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd1d2a48-5d7f-4ddd-98e9-8d757aa4c264",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd1d2a48-5d7f-4ddd-98e9-8d757aa4c264",
        "outputId": "5155e4cc-b5f6-4004-9ef7-52171fb9a4bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20906, [1200, 16, 3200, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model = Net()\n",
        "\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac398c6-5560-4f14-a4e0-574b1b936dce",
      "metadata": {
        "id": "fac398c6-5560-4f14-a4e0-574b1b936dce"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "497e3bda-b356-4b43-a82f-c0c622e246d1",
      "metadata": {
        "id": "497e3bda-b356-4b43-a82f-c0c622e246d1"
      },
      "outputs": [],
      "source": [
        "import datetime  # <1>\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):  # <2>\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:  # <3>\n",
        "\n",
        "            outputs = model(imgs)  # <4>\n",
        "\n",
        "            loss = loss_fn(outputs, labels)  # <5>\n",
        "\n",
        "            optimizer.zero_grad()  # <6>\n",
        "\n",
        "            loss.backward()  # <7>\n",
        "\n",
        "            optimizer.step()  # <8>\n",
        "\n",
        "            loss_train += loss.item()  # <9>\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))  # <10>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the Dataset from chapter 7; wrap it into a DataLoader; instantiate our network, an optimizer, and a loss function as before; and call our training loop.\n",
        " The substantial changes in our model from the last chapter are that now our\n",
        "model is a custom subclass of nn.Module and that we’re using convolutions. Let’s run\n",
        "training for 100 epochs while printing the loss. Depending on your hardware, this\n",
        "may take 20 minutes or more to finish!"
      ],
      "metadata": {
        "id": "l7jWG1A3c_fi"
      },
      "id": "l7jWG1A3c_fi"
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)  # <1>\n",
        "\n",
        "model = Net()  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItjS8PF5cvck",
        "outputId": "a48c2a15-60bb-4bb4-f100-6a0597b1ed18"
      },
      "id": "ItjS8PF5cvck",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-26 17:51:44.178096 Epoch 1, Training loss 0.5639230434302311\n",
            "2024-04-26 17:52:37.200346 Epoch 10, Training loss 0.3269234761880462\n",
            "2024-04-26 17:53:35.923911 Epoch 20, Training loss 0.2844390675520441\n",
            "2024-04-26 17:54:34.117760 Epoch 30, Training loss 0.2556565905072887\n",
            "2024-04-26 17:55:33.628358 Epoch 40, Training loss 0.2317938423555368\n",
            "2024-04-26 17:56:31.910825 Epoch 50, Training loss 0.2051310299118613\n",
            "2024-04-26 17:57:30.688470 Epoch 60, Training loss 0.18354635097228797\n",
            "2024-04-26 17:58:29.351167 Epoch 70, Training loss 0.1645348866464226\n",
            "2024-04-26 17:59:28.720147 Epoch 80, Training loss 0.14300961151814004\n",
            "2024-04-26 18:00:27.506357 Epoch 90, Training loss 0.12541566530515433\n",
            "2024-04-26 18:01:26.332072 Epoch 100, Training loss 0.10656613130478343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Training and Validation Loss to see the amount of overfitting"
      ],
      "metadata": {
        "id": "6SFNaP_HeBxq"
      },
      "id": "6SFNaP_HeBxq"
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # <1>\n",
        "            for imgs, labels in loader:\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # <2>\n",
        "                total += labels.shape[0]  # <3>\n",
        "                correct += int((predicted == labels).sum())  # <4>\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "UI6L8LiGmY7L",
        "outputId": "f081af52-0730-48b7-b70d-a3454f7e4bfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UI6L8LiGmY7L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.96\n",
            "Accuracy val: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernel_size can be a tuple of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimension\n",
        "\n",
        "There is a slight increase in the training accuracy but no change in Validation accuracy when we change filter size from 3x3 to 5x5"
      ],
      "metadata": {
        "id": "t-pYqzgjmb08"
      },
      "id": "t-pYqzgjmb08"
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Kernel size (1,3) using padding = 'same' which pads the input so the output has the shape as the input. However, this mode doesn’t support any stride values other than 1."
      ],
      "metadata": {
        "id": "1NqUZRivW6Gw"
      },
      "id": "1NqUZRivW6Gw"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=(1,3), padding='same')\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=(1,3), padding='same')\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * 8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "p18tNak-njXK"
      },
      "id": "p18tNak-njXK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "metadata": {
        "id": "yJ1BjjjXMw9h",
        "outputId": "c82daea4-7665-49a2-8a15-55ff6aac8a08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yJ1BjjjXMw9h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17034, [144, 16, 384, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime  # <1>\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):  # <2>\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:  # <3>\n",
        "\n",
        "            outputs = model(imgs)  # <4>\n",
        "\n",
        "            loss = loss_fn(outputs, labels)  # <5>\n",
        "\n",
        "            optimizer.zero_grad()  # <6>\n",
        "\n",
        "            loss.backward()  # <7>\n",
        "\n",
        "            optimizer.step()  # <8>\n",
        "\n",
        "            loss_train += loss.item()  # <9>\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))  # <10>"
      ],
      "metadata": {
        "id": "AziqeAiZNGwz"
      },
      "id": "AziqeAiZNGwz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)  # <1>\n",
        "\n",
        "model = Net()  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "id": "wbBiyu84NNoa",
        "outputId": "9f76590c-bbfb-4dbc-a028-86c6a0a42619",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wbBiyu84NNoa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-26 21:26:11.400002 Epoch 1, Training loss 0.5659798695023652\n",
            "2024-04-26 21:26:50.290938 Epoch 10, Training loss 0.34299551254245125\n",
            "2024-04-26 21:27:31.790345 Epoch 20, Training loss 0.3158297742817812\n",
            "2024-04-26 21:28:13.024979 Epoch 30, Training loss 0.2944501834880015\n",
            "2024-04-26 21:28:55.054121 Epoch 40, Training loss 0.27680306620658585\n",
            "2024-04-26 21:29:36.148278 Epoch 50, Training loss 0.2629637216116972\n",
            "2024-04-26 21:30:17.983434 Epoch 60, Training loss 0.24467909226941456\n",
            "2024-04-26 21:30:58.740232 Epoch 70, Training loss 0.23126557146667676\n",
            "2024-04-26 21:31:40.665242 Epoch 80, Training loss 0.21842472611149405\n",
            "2024-04-26 21:32:22.553775 Epoch 90, Training loss 0.20714929903958254\n",
            "2024-04-26 21:33:04.600887 Epoch 100, Training loss 0.19390393776973341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # <1>\n",
        "            for imgs, labels in loader:\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # <2>\n",
        "                total += labels.shape[0]  # <3>\n",
        "                correct += int((predicted == labels).sum())  # <4>\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "T2S9O6aRNRiI",
        "outputId": "f72bf4bf-fa95-44de-84a6-0f9d7e4464e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "T2S9O6aRNRiI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.92\n",
            "Accuracy val: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chap 10 Deep Learning with Python"
      ],
      "metadata": {
        "id": "WPAaLqS9sx5Q"
      },
      "id": "WPAaLqS9sx5Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 1: Load and prepare the weather data and implement the model described in\n",
        "Listing 10.23 (two GRU layers). <br>\n",
        "Replace the model-check-point callback with an early-stopping callback;  <br>\n",
        "set the restore_best_weights parameter to True and the patience parameter to 5.  <br>\n",
        "Monitor the validation MAE."
      ],
      "metadata": {
        "id": "Of-TsTC5Wzyd"
      },
      "id": "Of-TsTC5Wzyd"
    },
    {
      "cell_type": "code",
      "source": [
        "#loading and preparing the weather data\n",
        "!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
        "!unzip jena_climate_2009_2016.csv.zip"
      ],
      "metadata": {
        "id": "0jkZm524N8im",
        "outputId": "a9dfe92b-bee3-4d0b-e2ad-0d05d19bee59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0jkZm524N8im",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-27 20:50:01--  https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.147.37, 52.217.136.184, 54.231.230.24, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.147.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13565642 (13M) [application/zip]\n",
            "Saving to: ‘jena_climate_2009_2016.csv.zip.1’\n",
            "\n",
            "jena_climate_2009_2 100%[===================>]  12.94M  44.8MB/s    in 0.3s    \n",
            "\n",
            "2024-04-27 20:50:02 (44.8 MB/s) - ‘jena_climate_2009_2016.csv.zip.1’ saved [13565642/13565642]\n",
            "\n",
            "Archive:  jena_climate_2009_2016.csv.zip\n",
            "replace jena_climate_2009_2016.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting the data of the Jena weather dataset"
      ],
      "metadata": {
        "id": "0SBr8OBHOvAM"
      },
      "id": "0SBr8OBHOvAM"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "fname = os.path.join(\"jena_climate_2009_2016.csv\")\n",
        "\n",
        "with open(fname) as f:\n",
        "    data = f.read()\n",
        "\n",
        "lines = data.split(\"\\n\")\n",
        "header = lines[0].split(\",\")\n",
        "lines = lines[1:]\n",
        "print(header)\n",
        "print(len(lines))"
      ],
      "metadata": {
        "id": "q0uFxe8fOkBh",
        "outputId": "c7627dc5-ed61-47d6-c91a-c26cd37bfe56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "q0uFxe8fOkBh",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n",
            "420451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing the data"
      ],
      "metadata": {
        "id": "RrYUwnauO_N2"
      },
      "id": "RrYUwnauO_N2"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "temperature = np.zeros((len(lines),))\n",
        "raw_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(\",\")[1:]]\n",
        "    temperature[i] = values[1]\n",
        "    raw_data[i, :] = values[:]"
      ],
      "metadata": {
        "id": "cpINoKikPNPY"
      },
      "id": "cpINoKikPNPY",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing the data"
      ],
      "metadata": {
        "id": "SrUIbzJ2PPox"
      },
      "id": "SrUIbzJ2PPox"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing the number of samples we'll use for each data split"
      ],
      "metadata": {
        "id": "1nK3VGMdPpzw"
      },
      "id": "1nK3VGMdPpzw"
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_samples = int(0.5 * len(raw_data))\n",
        "num_val_samples = int(0.25 * len(raw_data))\n",
        "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n",
        "print(\"num_train_samples:\", num_train_samples)\n",
        "print(\"num_val_samples:\", num_val_samples)\n",
        "print(\"num_test_samples:\", num_test_samples)"
      ],
      "metadata": {
        "id": "KyyeXQ1oPujX",
        "outputId": "d89527ed-d797-4c29-8fb9-6c8fda96f5ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KyyeXQ1oPujX",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_train_samples: 210225\n",
            "num_val_samples: 105112\n",
            "num_test_samples: 105114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing the data"
      ],
      "metadata": {
        "id": "9U4VFYT6P2Fg"
      },
      "id": "9U4VFYT6P2Fg"
    },
    {
      "cell_type": "code",
      "source": [
        "mean = raw_data[:num_train_samples].mean(axis=0)\n",
        "raw_data -= mean\n",
        "std = raw_data[:num_train_samples].std(axis=0)\n",
        "raw_data /= std"
      ],
      "metadata": {
        "id": "3ucRuBADPdtv"
      },
      "id": "3ucRuBADPdtv",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiating datasets for training, validation, and testing"
      ],
      "metadata": {
        "id": "LmI4QM8PPgsq"
      },
      "id": "LmI4QM8PPgsq"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "sampling_rate = 6\n",
        "sequence_length = 120\n",
        "delay = sampling_rate * (sequence_length + 24 - 1)\n",
        "batch_size = 256\n",
        "\n",
        "train_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=0,\n",
        "    end_index=num_train_samples)\n",
        "\n",
        "val_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples,\n",
        "    end_index=num_train_samples + num_val_samples)\n",
        "\n",
        "test_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples + num_val_samples)"
      ],
      "metadata": {
        "id": "zGdth5UeQHUg"
      },
      "id": "zGdth5UeQHUg",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing the common-sense baseline MAE"
      ],
      "metadata": {
        "id": "srULJlXtQN3Z"
      },
      "id": "srULJlXtQN3Z"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_naive_method(dataset):\n",
        "    total_abs_err = 0.\n",
        "    samples_seen = 0\n",
        "    for samples, targets in dataset:\n",
        "        preds = samples[:, -1, 1] * std[1] + mean[1]\n",
        "        total_abs_err += np.sum(np.abs(preds - targets))\n",
        "        samples_seen += samples.shape[0]\n",
        "    return total_abs_err / samples_seen\n",
        "\n",
        "print(f\"Validation MAE: {evaluate_naive_method(val_dataset):.2f}\")\n",
        "print(f\"Test MAE: {evaluate_naive_method(test_dataset):.2f}\")"
      ],
      "metadata": {
        "id": "hMIPmansQ83I",
        "outputId": "a9bdad2d-5891-4e04-aaa5-3707f8be14f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hMIPmansQ83I",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MAE: 2.44\n",
            "Test MAE: 2.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stacking recurrent layers"
      ],
      "metadata": {
        "id": "31fhaccaQ_Ir"
      },
      "id": "31fhaccaQ_Ir"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Training and evaluating a dropout-regularized, stacked GRU model**<br>\n",
        "**Experiment 1**<br>\n",
        "Replace the model-check-point callback with an early-stopping callback; set the restore_best_weights parameter to True and the patience\n",
        "parameter to 5. Monitor the validation MAE."
      ],
      "metadata": {
        "id": "ItyncZNqRhRE"
      },
      "id": "ItyncZNqRhRE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "EarlyStopping : Stop training when a monitored metric has stopped improving.<br>\n",
        "<br> Test_mae value greater than the original GRU model after changing to early stopping. Original GRU model has 2.39 as test_mae value"
      ],
      "metadata": {
        "id": "yd8af5uyT3DX"
      },
      "id": "yd8af5uyT3DX"
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)\n",
        "x = layers.GRU(32, recurrent_dropout=0.5)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping( monitor=\"val_mae\",patience=5,restore_best_weights=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ],
      "metadata": {
        "id": "Cj6W6vK_RwJP",
        "outputId": "d42e8abb-8cbd-4e94-eb61-248f088c9c17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Cj6W6vK_RwJP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "819/819 [==============================] - 187s 225ms/step - loss: 28.2655 - mae: 3.9035 - val_loss: 9.2553 - val_mae: 2.3457\n",
            "Epoch 2/50\n",
            "819/819 [==============================] - 191s 233ms/step - loss: 14.0323 - mae: 2.9054 - val_loss: 9.2471 - val_mae: 2.3594\n",
            "Epoch 3/50\n",
            "819/819 [==============================] - 189s 231ms/step - loss: 13.2106 - mae: 2.8194 - val_loss: 9.3891 - val_mae: 2.3792\n",
            "Epoch 4/50\n",
            "819/819 [==============================] - 189s 230ms/step - loss: 12.7239 - mae: 2.7673 - val_loss: 9.0997 - val_mae: 2.3381\n",
            "Epoch 5/50\n",
            "819/819 [==============================] - 187s 229ms/step - loss: 12.1535 - mae: 2.7027 - val_loss: 8.8078 - val_mae: 2.3117\n",
            "Epoch 6/50\n",
            "819/819 [==============================] - 189s 231ms/step - loss: 11.7367 - mae: 2.6605 - val_loss: 9.4152 - val_mae: 2.3940\n",
            "Epoch 7/50\n",
            "819/819 [==============================] - 187s 229ms/step - loss: 11.4010 - mae: 2.6232 - val_loss: 8.9971 - val_mae: 2.3470\n",
            "Epoch 8/50\n",
            "819/819 [==============================] - 187s 228ms/step - loss: 11.0140 - mae: 2.5779 - val_loss: 8.8801 - val_mae: 2.3162\n",
            "Epoch 9/50\n",
            "819/819 [==============================] - 191s 233ms/step - loss: 10.6830 - mae: 2.5395 - val_loss: 8.9042 - val_mae: 2.3328\n",
            "Epoch 10/50\n",
            "819/819 [==============================] - 190s 232ms/step - loss: 10.3618 - mae: 2.5016 - val_loss: 8.7669 - val_mae: 2.3098\n",
            "Epoch 11/50\n",
            "819/819 [==============================] - 187s 229ms/step - loss: 10.1401 - mae: 2.4756 - val_loss: 8.9031 - val_mae: 2.3277\n",
            "Epoch 12/50\n",
            "819/819 [==============================] - 192s 235ms/step - loss: 9.8808 - mae: 2.4459 - val_loss: 9.3397 - val_mae: 2.3929\n",
            "Epoch 13/50\n",
            "819/819 [==============================] - 194s 236ms/step - loss: 9.6811 - mae: 2.4204 - val_loss: 10.2499 - val_mae: 2.5060\n",
            "Epoch 14/50\n",
            "819/819 [==============================] - 196s 240ms/step - loss: 9.4684 - mae: 2.3928 - val_loss: 10.0537 - val_mae: 2.4785\n",
            "Epoch 15/50\n",
            "819/819 [==============================] - 195s 238ms/step - loss: 9.3022 - mae: 2.3730 - val_loss: 10.2053 - val_mae: 2.4894\n",
            "405/405 [==============================] - 27s 66ms/step - loss: 9.7947 - mae: 2.4723\n",
            "Test MAE: 2.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Experiment 2: Adjust the number of units in each recurrent layer in the stacked setup, as well as the amount of dropout\n",
        "\n",
        "units: Positive integer, dimensionality of the output space.<br>\n",
        "units changed to 16 and recurrent_dropout and dropout changed to 0.3<br>\n",
        "Observation: Test_mae value greater than the original GRU model and no significant change from Experiment 1.<br>\n"
      ],
      "metadata": {
        "id": "Y-m43R3pVE0W"
      },
      "id": "Y-m43R3pVE0W"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.GRU(16, recurrent_dropout=0.3, return_sequences=True)(inputs)\n",
        "x = layers.GRU(16, recurrent_dropout=0.3)(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_stacked_gru_dropout.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "model = keras.models.load_model(\"jena_stacked_gru_dropout.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYHwAa0_TBkh",
        "outputId": "1bb0671d-ab06-41f7-925b-b3e0c665ad03"
      },
      "id": "RYHwAa0_TBkh",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "819/819 [==============================] - 159s 189ms/step - loss: 39.8764 - mae: 4.6351 - val_loss: 12.2673 - val_mae: 2.6216\n",
            "Epoch 2/50\n",
            "819/819 [==============================] - 139s 169ms/step - loss: 14.3318 - mae: 2.9289 - val_loss: 9.1999 - val_mae: 2.3433\n",
            "Epoch 3/50\n",
            "819/819 [==============================] - 139s 170ms/step - loss: 13.0486 - mae: 2.8040 - val_loss: 9.0523 - val_mae: 2.3318\n",
            "Epoch 4/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 12.5551 - mae: 2.7477 - val_loss: 8.9762 - val_mae: 2.3191\n",
            "Epoch 5/50\n",
            "819/819 [==============================] - 144s 175ms/step - loss: 12.1766 - mae: 2.7072 - val_loss: 9.0265 - val_mae: 2.3324\n",
            "Epoch 6/50\n",
            "819/819 [==============================] - 148s 181ms/step - loss: 11.8159 - mae: 2.6697 - val_loss: 8.6915 - val_mae: 2.2958\n",
            "Epoch 7/50\n",
            "819/819 [==============================] - 146s 178ms/step - loss: 11.5321 - mae: 2.6405 - val_loss: 8.9875 - val_mae: 2.3314\n",
            "Epoch 8/50\n",
            "819/819 [==============================] - 147s 179ms/step - loss: 11.2034 - mae: 2.6013 - val_loss: 9.1916 - val_mae: 2.3591\n",
            "Epoch 9/50\n",
            "819/819 [==============================] - 142s 173ms/step - loss: 10.9345 - mae: 2.5721 - val_loss: 9.0894 - val_mae: 2.3555\n",
            "Epoch 10/50\n",
            "819/819 [==============================] - 141s 172ms/step - loss: 10.7431 - mae: 2.5524 - val_loss: 8.8944 - val_mae: 2.3296\n",
            "Epoch 11/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 10.4935 - mae: 2.5241 - val_loss: 8.8584 - val_mae: 2.3235\n",
            "Epoch 12/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 10.3815 - mae: 2.5089 - val_loss: 8.8087 - val_mae: 2.3102\n",
            "Epoch 13/50\n",
            "819/819 [==============================] - 139s 170ms/step - loss: 10.2358 - mae: 2.4919 - val_loss: 9.2268 - val_mae: 2.3715\n",
            "Epoch 14/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 10.1041 - mae: 2.4778 - val_loss: 9.3117 - val_mae: 2.3895\n",
            "Epoch 15/50\n",
            "819/819 [==============================] - 138s 168ms/step - loss: 9.9811 - mae: 2.4629 - val_loss: 9.3547 - val_mae: 2.3866\n",
            "Epoch 16/50\n",
            "819/819 [==============================] - 138s 169ms/step - loss: 9.8687 - mae: 2.4486 - val_loss: 9.3020 - val_mae: 2.3847\n",
            "Epoch 17/50\n",
            "819/819 [==============================] - 138s 168ms/step - loss: 9.7777 - mae: 2.4369 - val_loss: 9.4296 - val_mae: 2.4049\n",
            "Epoch 18/50\n",
            "819/819 [==============================] - 138s 168ms/step - loss: 9.6559 - mae: 2.4212 - val_loss: 9.6074 - val_mae: 2.4192\n",
            "Epoch 19/50\n",
            "819/819 [==============================] - 139s 169ms/step - loss: 9.5908 - mae: 2.4148 - val_loss: 9.8707 - val_mae: 2.4556\n",
            "Epoch 20/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 9.4829 - mae: 2.4021 - val_loss: 9.7845 - val_mae: 2.4537\n",
            "Epoch 21/50\n",
            "819/819 [==============================] - 139s 169ms/step - loss: 9.3638 - mae: 2.3878 - val_loss: 10.1913 - val_mae: 2.4996\n",
            "Epoch 22/50\n",
            "819/819 [==============================] - 140s 170ms/step - loss: 9.3320 - mae: 2.3831 - val_loss: 10.1360 - val_mae: 2.4920\n",
            "Epoch 23/50\n",
            "819/819 [==============================] - 141s 172ms/step - loss: 9.2289 - mae: 2.3692 - val_loss: 10.4250 - val_mae: 2.5195\n",
            "Epoch 24/50\n",
            "819/819 [==============================] - 142s 173ms/step - loss: 9.2057 - mae: 2.3681 - val_loss: 10.1650 - val_mae: 2.4880\n",
            "Epoch 25/50\n",
            "819/819 [==============================] - 144s 176ms/step - loss: 9.1154 - mae: 2.3559 - val_loss: 10.4241 - val_mae: 2.5217\n",
            "Epoch 26/50\n",
            "819/819 [==============================] - 144s 176ms/step - loss: 9.0864 - mae: 2.3537 - val_loss: 10.6803 - val_mae: 2.5506\n",
            "Epoch 27/50\n",
            "819/819 [==============================] - 144s 175ms/step - loss: 8.9841 - mae: 2.3365 - val_loss: 10.2161 - val_mae: 2.4966\n",
            "Epoch 28/50\n",
            "819/819 [==============================] - 151s 184ms/step - loss: 8.9278 - mae: 2.3323 - val_loss: 10.6990 - val_mae: 2.5543\n",
            "Epoch 29/50\n",
            "819/819 [==============================] - 151s 185ms/step - loss: 8.8786 - mae: 2.3263 - val_loss: 10.5162 - val_mae: 2.5280\n",
            "Epoch 30/50\n",
            "819/819 [==============================] - 146s 177ms/step - loss: 8.8267 - mae: 2.3199 - val_loss: 10.6955 - val_mae: 2.5515\n",
            "Epoch 31/50\n",
            "819/819 [==============================] - 150s 183ms/step - loss: 8.7318 - mae: 2.3072 - val_loss: 10.7344 - val_mae: 2.5575\n",
            "Epoch 32/50\n",
            "819/819 [==============================] - 145s 177ms/step - loss: 8.6886 - mae: 2.3015 - val_loss: 10.8880 - val_mae: 2.5779\n",
            "Epoch 33/50\n",
            "819/819 [==============================] - 173s 211ms/step - loss: 8.6433 - mae: 2.2952 - val_loss: 11.0735 - val_mae: 2.5972\n",
            "Epoch 34/50\n",
            "819/819 [==============================] - 148s 180ms/step - loss: 8.5993 - mae: 2.2907 - val_loss: 11.0310 - val_mae: 2.5901\n",
            "Epoch 35/50\n",
            "819/819 [==============================] - 143s 175ms/step - loss: 8.5863 - mae: 2.2858 - val_loss: 11.0411 - val_mae: 2.5940\n",
            "Epoch 36/50\n",
            "819/819 [==============================] - 145s 177ms/step - loss: 8.5590 - mae: 2.2835 - val_loss: 11.0833 - val_mae: 2.5999\n",
            "Epoch 37/50\n",
            "819/819 [==============================] - 145s 176ms/step - loss: 8.4990 - mae: 2.2756 - val_loss: 11.1924 - val_mae: 2.6217\n",
            "Epoch 38/50\n",
            "819/819 [==============================] - 163s 199ms/step - loss: 8.4449 - mae: 2.2687 - val_loss: 11.6336 - val_mae: 2.6691\n",
            "Epoch 39/50\n",
            "819/819 [==============================] - 141s 172ms/step - loss: 8.4052 - mae: 2.2637 - val_loss: 11.2441 - val_mae: 2.6187\n",
            "Epoch 40/50\n",
            "819/819 [==============================] - 140s 170ms/step - loss: 8.4070 - mae: 2.2644 - val_loss: 11.5902 - val_mae: 2.6694\n",
            "Epoch 41/50\n",
            "819/819 [==============================] - 139s 170ms/step - loss: 8.3665 - mae: 2.2574 - val_loss: 11.4612 - val_mae: 2.6519\n",
            "Epoch 42/50\n",
            "819/819 [==============================] - 138s 169ms/step - loss: 8.3112 - mae: 2.2520 - val_loss: 11.5068 - val_mae: 2.6581\n",
            "Epoch 43/50\n",
            "819/819 [==============================] - 138s 169ms/step - loss: 8.2856 - mae: 2.2474 - val_loss: 11.4078 - val_mae: 2.6445\n",
            "Epoch 44/50\n",
            "819/819 [==============================] - 142s 173ms/step - loss: 8.2436 - mae: 2.2426 - val_loss: 11.5805 - val_mae: 2.6577\n",
            "Epoch 45/50\n",
            "819/819 [==============================] - 143s 174ms/step - loss: 8.2647 - mae: 2.2449 - val_loss: 11.3945 - val_mae: 2.6416\n",
            "Epoch 46/50\n",
            "819/819 [==============================] - 144s 176ms/step - loss: 8.2135 - mae: 2.2372 - val_loss: 11.5788 - val_mae: 2.6578\n",
            "Epoch 47/50\n",
            "819/819 [==============================] - 141s 172ms/step - loss: 8.2110 - mae: 2.2364 - val_loss: 11.9130 - val_mae: 2.7076\n",
            "Epoch 48/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 8.1554 - mae: 2.2310 - val_loss: 11.5887 - val_mae: 2.6674\n",
            "Epoch 49/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 8.1476 - mae: 2.2295 - val_loss: 11.5778 - val_mae: 2.6613\n",
            "Epoch 50/50\n",
            "819/819 [==============================] - 139s 170ms/step - loss: 8.1245 - mae: 2.2249 - val_loss: 11.8192 - val_mae: 2.6908\n",
            "405/405 [==============================] - 21s 49ms/step - loss: 9.7268 - mae: 2.4484\n",
            "Test MAE: 2.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Experiment 3: Adjust the learning rate used by the RMSprop optimizer, or try a\n",
        "different optimizer, e.g., Adam."
      ],
      "metadata": {
        "id": "m7BpnG8ITnYj"
      },
      "id": "m7BpnG8ITnYj"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.GRU(16, recurrent_dropout=0.3, return_sequences=True)(inputs)\n",
        "x = layers.GRU(16, recurrent_dropout=0.3)(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_stacked_gru_dropout.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "model = keras.models.load_model(\"jena_stacked_gru_dropout.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ],
      "metadata": {
        "id": "AuccP-Ot6IPj",
        "outputId": "1871accc-f7c8-4efe-b1c3-55b92d510daf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AuccP-Ot6IPj",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "819/819 [==============================] - 154s 183ms/step - loss: 46.2343 - mae: 4.9966 - val_loss: 18.6787 - val_mae: 3.1482\n",
            "Epoch 2/50\n",
            "819/819 [==============================] - 153s 186ms/step - loss: 17.4435 - mae: 3.1564 - val_loss: 11.6887 - val_mae: 2.5635\n",
            "Epoch 3/50\n",
            "819/819 [==============================] - 153s 187ms/step - loss: 14.3600 - mae: 2.9117 - val_loss: 10.3212 - val_mae: 2.4500\n",
            "Epoch 4/50\n",
            "819/819 [==============================] - 152s 185ms/step - loss: 13.3287 - mae: 2.8201 - val_loss: 9.8207 - val_mae: 2.4097\n",
            "Epoch 5/50\n",
            "819/819 [==============================] - 153s 187ms/step - loss: 12.7641 - mae: 2.7632 - val_loss: 9.5004 - val_mae: 2.3844\n",
            "Epoch 6/50\n",
            "819/819 [==============================] - 152s 185ms/step - loss: 12.4051 - mae: 2.7269 - val_loss: 9.8322 - val_mae: 2.4328\n",
            "Epoch 7/50\n",
            "819/819 [==============================] - 149s 182ms/step - loss: 12.0690 - mae: 2.6920 - val_loss: 9.2135 - val_mae: 2.3590\n",
            "Epoch 8/50\n",
            "819/819 [==============================] - 148s 181ms/step - loss: 11.6992 - mae: 2.6520 - val_loss: 9.3870 - val_mae: 2.3778\n",
            "Epoch 9/50\n",
            "819/819 [==============================] - 146s 178ms/step - loss: 11.3975 - mae: 2.6184 - val_loss: 9.4680 - val_mae: 2.3926\n",
            "Epoch 10/50\n",
            "819/819 [==============================] - 146s 178ms/step - loss: 11.1267 - mae: 2.5914 - val_loss: 9.4907 - val_mae: 2.3934\n",
            "Epoch 11/50\n",
            "819/819 [==============================] - 151s 185ms/step - loss: 10.8956 - mae: 2.5649 - val_loss: 9.1921 - val_mae: 2.3614\n",
            "Epoch 12/50\n",
            "819/819 [==============================] - 152s 185ms/step - loss: 10.7788 - mae: 2.5500 - val_loss: 9.2109 - val_mae: 2.3637\n",
            "Epoch 13/50\n",
            "819/819 [==============================] - 148s 180ms/step - loss: 10.4964 - mae: 2.5187 - val_loss: 9.2424 - val_mae: 2.3674\n",
            "Epoch 14/50\n",
            "819/819 [==============================] - 149s 182ms/step - loss: 10.3483 - mae: 2.5029 - val_loss: 9.5171 - val_mae: 2.4066\n",
            "Epoch 15/50\n",
            "819/819 [==============================] - 145s 177ms/step - loss: 10.1762 - mae: 2.4790 - val_loss: 9.4418 - val_mae: 2.3977\n",
            "Epoch 16/50\n",
            "819/819 [==============================] - 145s 177ms/step - loss: 10.0679 - mae: 2.4681 - val_loss: 9.5398 - val_mae: 2.4126\n",
            "Epoch 17/50\n",
            "819/819 [==============================] - 146s 177ms/step - loss: 9.8823 - mae: 2.4481 - val_loss: 9.4202 - val_mae: 2.3942\n",
            "Epoch 18/50\n",
            "819/819 [==============================] - 148s 181ms/step - loss: 9.8184 - mae: 2.4362 - val_loss: 9.5816 - val_mae: 2.4241\n",
            "Epoch 19/50\n",
            "819/819 [==============================] - 146s 178ms/step - loss: 9.6819 - mae: 2.4231 - val_loss: 9.6322 - val_mae: 2.4271\n",
            "Epoch 20/50\n",
            "819/819 [==============================] - 146s 178ms/step - loss: 9.5998 - mae: 2.4127 - val_loss: 9.5757 - val_mae: 2.4219\n",
            "Epoch 21/50\n",
            "819/819 [==============================] - 145s 177ms/step - loss: 9.5204 - mae: 2.4024 - val_loss: 9.8807 - val_mae: 2.4657\n",
            "Epoch 22/50\n",
            "819/819 [==============================] - 145s 177ms/step - loss: 9.5698 - mae: 2.4078 - val_loss: 9.8016 - val_mae: 2.4546\n",
            "Epoch 23/50\n",
            "819/819 [==============================] - 146s 177ms/step - loss: 9.4002 - mae: 2.3849 - val_loss: 10.0165 - val_mae: 2.4820\n",
            "Epoch 24/50\n",
            "819/819 [==============================] - 143s 174ms/step - loss: 9.3849 - mae: 2.3833 - val_loss: 10.0013 - val_mae: 2.4781\n",
            "Epoch 25/50\n",
            "819/819 [==============================] - 143s 175ms/step - loss: 9.2711 - mae: 2.3702 - val_loss: 9.8288 - val_mae: 2.4589\n",
            "Epoch 26/50\n",
            "819/819 [==============================] - 146s 178ms/step - loss: 9.2326 - mae: 2.3650 - val_loss: 10.0724 - val_mae: 2.4889\n",
            "Epoch 27/50\n",
            "819/819 [==============================] - 142s 174ms/step - loss: 9.1498 - mae: 2.3577 - val_loss: 10.1472 - val_mae: 2.4989\n",
            "Epoch 28/50\n",
            "819/819 [==============================] - 143s 175ms/step - loss: 9.0871 - mae: 2.3494 - val_loss: 10.1992 - val_mae: 2.5060\n",
            "Epoch 29/50\n",
            "819/819 [==============================] - 157s 192ms/step - loss: 9.1002 - mae: 2.3490 - val_loss: 10.3365 - val_mae: 2.5237\n",
            "Epoch 30/50\n",
            "819/819 [==============================] - 159s 194ms/step - loss: 8.9996 - mae: 2.3357 - val_loss: 10.2972 - val_mae: 2.5142\n",
            "Epoch 31/50\n",
            "819/819 [==============================] - 173s 211ms/step - loss: 8.9954 - mae: 2.3382 - val_loss: 10.3981 - val_mae: 2.5343\n",
            "Epoch 32/50\n",
            "819/819 [==============================] - 158s 193ms/step - loss: 8.9255 - mae: 2.3250 - val_loss: 10.3388 - val_mae: 2.5196\n",
            "Epoch 33/50\n",
            "819/819 [==============================] - 153s 187ms/step - loss: 8.8893 - mae: 2.3238 - val_loss: 10.4818 - val_mae: 2.5411\n",
            "Epoch 34/50\n",
            "819/819 [==============================] - 147s 179ms/step - loss: 8.8405 - mae: 2.3176 - val_loss: 10.5426 - val_mae: 2.5468\n",
            "Epoch 35/50\n",
            "819/819 [==============================] - 145s 177ms/step - loss: 8.8017 - mae: 2.3099 - val_loss: 10.5666 - val_mae: 2.5541\n",
            "Epoch 36/50\n",
            "819/819 [==============================] - 151s 185ms/step - loss: 8.7320 - mae: 2.3002 - val_loss: 10.6353 - val_mae: 2.5598\n",
            "Epoch 37/50\n",
            "819/819 [==============================] - 152s 185ms/step - loss: 8.6460 - mae: 2.2938 - val_loss: 10.9288 - val_mae: 2.6010\n",
            "Epoch 38/50\n",
            "819/819 [==============================] - 153s 187ms/step - loss: 8.6576 - mae: 2.2900 - val_loss: 10.8512 - val_mae: 2.5928\n",
            "Epoch 39/50\n",
            "819/819 [==============================] - 151s 184ms/step - loss: 8.6158 - mae: 2.2891 - val_loss: 10.7980 - val_mae: 2.5790\n",
            "Epoch 40/50\n",
            "819/819 [==============================] - 147s 179ms/step - loss: 8.6087 - mae: 2.2837 - val_loss: 10.9294 - val_mae: 2.6037\n",
            "Epoch 41/50\n",
            "819/819 [==============================] - 147s 179ms/step - loss: 8.5374 - mae: 2.2782 - val_loss: 10.9612 - val_mae: 2.6021\n",
            "Epoch 42/50\n",
            "819/819 [==============================] - 157s 192ms/step - loss: 8.5152 - mae: 2.2737 - val_loss: 10.9081 - val_mae: 2.5962\n",
            "Epoch 43/50\n",
            "819/819 [==============================] - 158s 192ms/step - loss: 8.4610 - mae: 2.2669 - val_loss: 11.1965 - val_mae: 2.6272\n",
            "Epoch 44/50\n",
            "819/819 [==============================] - 146s 178ms/step - loss: 8.4428 - mae: 2.2678 - val_loss: 11.1109 - val_mae: 2.6194\n",
            "Epoch 45/50\n",
            "819/819 [==============================] - 154s 187ms/step - loss: 8.4088 - mae: 2.2623 - val_loss: 11.0075 - val_mae: 2.6062\n",
            "Epoch 46/50\n",
            "819/819 [==============================] - 154s 187ms/step - loss: 8.3748 - mae: 2.2546 - val_loss: 11.2889 - val_mae: 2.6440\n",
            "Epoch 47/50\n",
            "819/819 [==============================] - 144s 176ms/step - loss: 8.3494 - mae: 2.2535 - val_loss: 11.1616 - val_mae: 2.6226\n",
            "Epoch 48/50\n",
            "819/819 [==============================] - 148s 181ms/step - loss: 8.3304 - mae: 2.2518 - val_loss: 11.2669 - val_mae: 2.6368\n",
            "Epoch 49/50\n",
            "819/819 [==============================] - 148s 180ms/step - loss: 8.3143 - mae: 2.2475 - val_loss: 11.0575 - val_mae: 2.6071\n",
            "Epoch 50/50\n",
            "819/819 [==============================] - 149s 181ms/step - loss: 8.2691 - mae: 2.2407 - val_loss: 11.3101 - val_mae: 2.6398\n",
            "405/405 [==============================] - 20s 47ms/step - loss: 9.9412 - mae: 2.4845\n",
            "Test MAE: 2.48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Experiment 4: Try using a stack of Dense layers as the regressor on top of the recurrent layer, instead of a single Dense layer."
      ],
      "metadata": {
        "id": "ME2_oGr08L-j"
      },
      "id": "ME2_oGr08L-j"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)\n",
        "x = layers.GRU(32, recurrent_dropout=0.5)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(16)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(8)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping( monitor=\"val_mae\",patience=5,restore_best_weights=True)\n",
        "]\n",
        "model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=25,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ],
      "metadata": {
        "id": "zbaHUOBHdmhn",
        "outputId": "f2ca9690-39ad-4519-931a-362f41abd261",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zbaHUOBHdmhn",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "819/819 [==============================] - 204s 241ms/step - loss: 44.9574 - mae: 5.0454 - val_loss: 12.8786 - val_mae: 2.7985\n",
            "Epoch 2/25\n",
            "819/819 [==============================] - 195s 237ms/step - loss: 33.4972 - mae: 4.3675 - val_loss: 13.1244 - val_mae: 2.8335\n",
            "Epoch 3/25\n",
            "819/819 [==============================] - 195s 238ms/step - loss: 30.6107 - mae: 4.1887 - val_loss: 12.0087 - val_mae: 2.7181\n",
            "Epoch 4/25\n",
            "819/819 [==============================] - 190s 232ms/step - loss: 28.9354 - mae: 4.0811 - val_loss: 11.0847 - val_mae: 2.5975\n",
            "Epoch 5/25\n",
            "819/819 [==============================] - 185s 226ms/step - loss: 27.5296 - mae: 3.9826 - val_loss: 11.6425 - val_mae: 2.6663\n",
            "Epoch 6/25\n",
            "819/819 [==============================] - 187s 228ms/step - loss: 26.4273 - mae: 3.9021 - val_loss: 10.8786 - val_mae: 2.5678\n",
            "Epoch 7/25\n",
            "819/819 [==============================] - 190s 231ms/step - loss: 25.5345 - mae: 3.8341 - val_loss: 10.6083 - val_mae: 2.5346\n",
            "Epoch 8/25\n",
            "819/819 [==============================] - 187s 228ms/step - loss: 24.5909 - mae: 3.7601 - val_loss: 11.9589 - val_mae: 2.6973\n",
            "Epoch 9/25\n",
            "819/819 [==============================] - 187s 228ms/step - loss: 23.8236 - mae: 3.7011 - val_loss: 11.6531 - val_mae: 2.6323\n",
            "Epoch 10/25\n",
            "819/819 [==============================] - 188s 229ms/step - loss: 23.0892 - mae: 3.6454 - val_loss: 11.5131 - val_mae: 2.6468\n",
            "Epoch 11/25\n",
            "819/819 [==============================] - 188s 229ms/step - loss: 22.7106 - mae: 3.6085 - val_loss: 11.1672 - val_mae: 2.6021\n",
            "Epoch 12/25\n",
            "819/819 [==============================] - 190s 231ms/step - loss: 22.3352 - mae: 3.5691 - val_loss: 10.6299 - val_mae: 2.5390\n",
            "405/405 [==============================] - 26s 62ms/step - loss: 12.2582 - mae: 2.7402\n",
            "Test MAE: 2.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "test_mae= [2.62, 2.39, 2.47, 2.45, 2.48, 2.74]\n",
        "index = ['Baseline','Original_GRU', 'Expt1', 'Expt2','Expt3', 'Expt4']\n",
        "df = pd.DataFrame({'test_mae': test_mae}, index=index)\n",
        "ax = df.plot.bar(rot=0)\n",
        "plt.legend(loc='upper center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fteVdjyHdoHB",
        "outputId": "352dc35d-df15-42fb-f7c4-1836f001235c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "id": "fteVdjyHdoHB",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uiKdKQbdm4NC"
      },
      "id": "uiKdKQbdm4NC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}