{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3fc93d99-758c-4348-bb3e-8c777aa39728",
      "metadata": {
        "id": "3fc93d99-758c-4348-bb3e-8c777aa39728"
      },
      "source": [
        "Change our model to use a 5 × 5 kernel with kernel_size=5 passed to the nn.Conv2d constructor. <br>\n",
        "a What impact does this change have on the number of parameters in the model?<br>\n",
        "b Does the change improve or degrade overfitting?<br>\n",
        "c Read https://pytorch.org/docs/stable/nn.html#conv2d.<br>\n",
        "d Can you describe what kernel_size=(1,3) will do?<br>\n",
        "e How does the model behave with such a kernel?<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef56940c-c18c-4a06-b538-b2b0727a68d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef56940c-c18c-4a06-b538-b2b0727a68d0",
        "outputId": "c0620811-1916-4b2b-f84c-21c334dc11f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e6c98299a90>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da650d2f-60a5-4029-94b2-905866f7618a",
      "metadata": {
        "id": "da650d2f-60a5-4029-94b2-905866f7618a"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5b048c-0cb4-4b6e-8dc5-d3d0c7760423",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec5b048c-0cb4-4b6e-8dc5-d3d0c7760423",
        "outputId": "196d8932-0588-43cc-a92b-de1e0cd9c20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 60942676.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversioned/p1ch6/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "data_path = '../data-unversioned/p1ch6/'\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c34ffa-3664-475a-8f57-cca15cb0138a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5c34ffa-3664-475a-8f57-cca15cb0138a",
        "outputId": "89c26a6d-d1f1-4577-870d-ff32ffb694cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7cf5a6-df01-43eb-b317-54ce8cd6f01f",
      "metadata": {
        "id": "6d7cf5a6-df01-43eb-b317-54ce8cd6f01f"
      },
      "outputs": [],
      "source": [
        "label_map = {0: 0, 2: 1}\n",
        "class_names = ['airplane', 'bird']\n",
        "cifar2 = [(img, label_map[label])\n",
        "          for img, label in cifar10\n",
        "          if label in [0, 2]]\n",
        "cifar2_val = [(img, label_map[label])\n",
        "              for img, label in cifar10_val\n",
        "              if label in [0, 2]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c13b95-de9b-47b7-a016-f405cd0d6b53",
      "metadata": {
        "id": "71c13b95-de9b-47b7-a016-f405cd0d6b53"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula given for calculating the output size (one dimension) of a convolution is (W−F+2P)/S+1\n",
        "\n",
        "If you want to keep the output size same as the input size, you can equate (W−max(F,S)+2P)/S+1=W\n",
        "Solving this, when S=1\n",
        " gives:\n",
        "\n",
        "W−F+2P+1=W\n",
        "\n",
        "2P=F−1\n",
        "\n",
        "When S>1\n",
        ", you could solve the amount of padding needed to keep the output size same as the input size:\n",
        "\n",
        "2P=max(F,S)−S\n",
        "\n",
        "https://stats.stackexchange.com/questions/297678/how-to-calculate-optimal-zero-padding-for-convolutional-neural-networks"
      ],
      "metadata": {
        "id": "1MfTKAhylMkJ"
      },
      "id": "1MfTKAhylMkJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "089f677d-8037-4a47-81c2-d2c7a3bc1ffd",
      "metadata": {
        "id": "089f677d-8037-4a47-81c2-d2c7a3bc1ffd"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=5, padding=2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * 8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5187844-403e-4852-ab8b-23608f7ca2bb",
      "metadata": {
        "id": "f5187844-403e-4852-ab8b-23608f7ca2bb"
      },
      "source": [
        "Number of Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd1d2a48-5d7f-4ddd-98e9-8d757aa4c264",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd1d2a48-5d7f-4ddd-98e9-8d757aa4c264",
        "outputId": "5155e4cc-b5f6-4004-9ef7-52171fb9a4bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20906, [1200, 16, 3200, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model = Net()\n",
        "\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac398c6-5560-4f14-a4e0-574b1b936dce",
      "metadata": {
        "id": "fac398c6-5560-4f14-a4e0-574b1b936dce"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "497e3bda-b356-4b43-a82f-c0c622e246d1",
      "metadata": {
        "id": "497e3bda-b356-4b43-a82f-c0c622e246d1"
      },
      "outputs": [],
      "source": [
        "import datetime  # <1>\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):  # <2>\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:  # <3>\n",
        "\n",
        "            outputs = model(imgs)  # <4>\n",
        "\n",
        "            loss = loss_fn(outputs, labels)  # <5>\n",
        "\n",
        "            optimizer.zero_grad()  # <6>\n",
        "\n",
        "            loss.backward()  # <7>\n",
        "\n",
        "            optimizer.step()  # <8>\n",
        "\n",
        "            loss_train += loss.item()  # <9>\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))  # <10>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the Dataset from chapter 7; wrap it into a DataLoader; instantiate our network, an optimizer, and a loss function as before; and call our training loop.\n",
        " The substantial changes in our model from the last chapter are that now our\n",
        "model is a custom subclass of nn.Module and that we’re using convolutions. Let’s run\n",
        "training for 100 epochs while printing the loss. Depending on your hardware, this\n",
        "may take 20 minutes or more to finish!"
      ],
      "metadata": {
        "id": "l7jWG1A3c_fi"
      },
      "id": "l7jWG1A3c_fi"
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)  # <1>\n",
        "\n",
        "model = Net()  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItjS8PF5cvck",
        "outputId": "a48c2a15-60bb-4bb4-f100-6a0597b1ed18"
      },
      "id": "ItjS8PF5cvck",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-26 17:51:44.178096 Epoch 1, Training loss 0.5639230434302311\n",
            "2024-04-26 17:52:37.200346 Epoch 10, Training loss 0.3269234761880462\n",
            "2024-04-26 17:53:35.923911 Epoch 20, Training loss 0.2844390675520441\n",
            "2024-04-26 17:54:34.117760 Epoch 30, Training loss 0.2556565905072887\n",
            "2024-04-26 17:55:33.628358 Epoch 40, Training loss 0.2317938423555368\n",
            "2024-04-26 17:56:31.910825 Epoch 50, Training loss 0.2051310299118613\n",
            "2024-04-26 17:57:30.688470 Epoch 60, Training loss 0.18354635097228797\n",
            "2024-04-26 17:58:29.351167 Epoch 70, Training loss 0.1645348866464226\n",
            "2024-04-26 17:59:28.720147 Epoch 80, Training loss 0.14300961151814004\n",
            "2024-04-26 18:00:27.506357 Epoch 90, Training loss 0.12541566530515433\n",
            "2024-04-26 18:01:26.332072 Epoch 100, Training loss 0.10656613130478343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Training and Validation Loss to see the amount of overfitting"
      ],
      "metadata": {
        "id": "6SFNaP_HeBxq"
      },
      "id": "6SFNaP_HeBxq"
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # <1>\n",
        "            for imgs, labels in loader:\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # <2>\n",
        "                total += labels.shape[0]  # <3>\n",
        "                correct += int((predicted == labels).sum())  # <4>\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "UI6L8LiGmY7L",
        "outputId": "f081af52-0730-48b7-b70d-a3454f7e4bfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UI6L8LiGmY7L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.96\n",
            "Accuracy val: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernel_size can be a tuple of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimension\n",
        "\n",
        "There is a slight increase in the training accuracy but no change in Validation accuracy when we change filter size from 3x3 to 5x5"
      ],
      "metadata": {
        "id": "t-pYqzgjmb08"
      },
      "id": "t-pYqzgjmb08"
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Kernel size (1,3) using padding = 'same' which pads the input so the output has the shape as the input. However, this mode doesn’t support any stride values other than 1."
      ],
      "metadata": {
        "id": "1NqUZRivW6Gw"
      },
      "id": "1NqUZRivW6Gw"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=(1,3), padding='same')\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=(1,3), padding='same')\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * 8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "p18tNak-njXK"
      },
      "id": "p18tNak-njXK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "metadata": {
        "id": "yJ1BjjjXMw9h",
        "outputId": "c82daea4-7665-49a2-8a15-55ff6aac8a08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yJ1BjjjXMw9h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17034, [144, 16, 384, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime  # <1>\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):  # <2>\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:  # <3>\n",
        "\n",
        "            outputs = model(imgs)  # <4>\n",
        "\n",
        "            loss = loss_fn(outputs, labels)  # <5>\n",
        "\n",
        "            optimizer.zero_grad()  # <6>\n",
        "\n",
        "            loss.backward()  # <7>\n",
        "\n",
        "            optimizer.step()  # <8>\n",
        "\n",
        "            loss_train += loss.item()  # <9>\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))  # <10>"
      ],
      "metadata": {
        "id": "AziqeAiZNGwz"
      },
      "id": "AziqeAiZNGwz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)  # <1>\n",
        "\n",
        "model = Net()  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "id": "wbBiyu84NNoa",
        "outputId": "9f76590c-bbfb-4dbc-a028-86c6a0a42619",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wbBiyu84NNoa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-26 21:26:11.400002 Epoch 1, Training loss 0.5659798695023652\n",
            "2024-04-26 21:26:50.290938 Epoch 10, Training loss 0.34299551254245125\n",
            "2024-04-26 21:27:31.790345 Epoch 20, Training loss 0.3158297742817812\n",
            "2024-04-26 21:28:13.024979 Epoch 30, Training loss 0.2944501834880015\n",
            "2024-04-26 21:28:55.054121 Epoch 40, Training loss 0.27680306620658585\n",
            "2024-04-26 21:29:36.148278 Epoch 50, Training loss 0.2629637216116972\n",
            "2024-04-26 21:30:17.983434 Epoch 60, Training loss 0.24467909226941456\n",
            "2024-04-26 21:30:58.740232 Epoch 70, Training loss 0.23126557146667676\n",
            "2024-04-26 21:31:40.665242 Epoch 80, Training loss 0.21842472611149405\n",
            "2024-04-26 21:32:22.553775 Epoch 90, Training loss 0.20714929903958254\n",
            "2024-04-26 21:33:04.600887 Epoch 100, Training loss 0.19390393776973341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # <1>\n",
        "            for imgs, labels in loader:\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # <2>\n",
        "                total += labels.shape[0]  # <3>\n",
        "                correct += int((predicted == labels).sum())  # <4>\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "T2S9O6aRNRiI",
        "outputId": "f72bf4bf-fa95-44de-84a6-0f9d7e4464e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "T2S9O6aRNRiI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.92\n",
            "Accuracy val: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chap 10 Deep Learning with Python"
      ],
      "metadata": {
        "id": "WPAaLqS9sx5Q"
      },
      "id": "WPAaLqS9sx5Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 1: Load and prepare the weather data and implement the model described in\n",
        "Listing 10.23 (two GRU layers). <br>\n",
        "Replace the model-check-point callback with an early-stopping callback;  <br>\n",
        "set the restore_best_weights parameter to True and the patience parameter to 5.  <br>\n",
        "Monitor the validation MAE."
      ],
      "metadata": {
        "id": "Of-TsTC5Wzyd"
      },
      "id": "Of-TsTC5Wzyd"
    },
    {
      "cell_type": "code",
      "source": [
        "#loading and preparing the weather data\n",
        "!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
        "!unzip jena_climate_2009_2016.csv.zip"
      ],
      "metadata": {
        "id": "0jkZm524N8im",
        "outputId": "a9dfe92b-bee3-4d0b-e2ad-0d05d19bee59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0jkZm524N8im",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-27 20:50:01--  https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.147.37, 52.217.136.184, 54.231.230.24, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.147.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13565642 (13M) [application/zip]\n",
            "Saving to: ‘jena_climate_2009_2016.csv.zip.1’\n",
            "\n",
            "jena_climate_2009_2 100%[===================>]  12.94M  44.8MB/s    in 0.3s    \n",
            "\n",
            "2024-04-27 20:50:02 (44.8 MB/s) - ‘jena_climate_2009_2016.csv.zip.1’ saved [13565642/13565642]\n",
            "\n",
            "Archive:  jena_climate_2009_2016.csv.zip\n",
            "replace jena_climate_2009_2016.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting the data of the Jena weather dataset"
      ],
      "metadata": {
        "id": "0SBr8OBHOvAM"
      },
      "id": "0SBr8OBHOvAM"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "fname = os.path.join(\"jena_climate_2009_2016.csv\")\n",
        "\n",
        "with open(fname) as f:\n",
        "    data = f.read()\n",
        "\n",
        "lines = data.split(\"\\n\")\n",
        "header = lines[0].split(\",\")\n",
        "lines = lines[1:]\n",
        "print(header)\n",
        "print(len(lines))"
      ],
      "metadata": {
        "id": "q0uFxe8fOkBh",
        "outputId": "c7627dc5-ed61-47d6-c91a-c26cd37bfe56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "q0uFxe8fOkBh",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n",
            "420451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing the data"
      ],
      "metadata": {
        "id": "RrYUwnauO_N2"
      },
      "id": "RrYUwnauO_N2"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "temperature = np.zeros((len(lines),))\n",
        "raw_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(\",\")[1:]]\n",
        "    temperature[i] = values[1]\n",
        "    raw_data[i, :] = values[:]"
      ],
      "metadata": {
        "id": "cpINoKikPNPY"
      },
      "id": "cpINoKikPNPY",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing the data"
      ],
      "metadata": {
        "id": "SrUIbzJ2PPox"
      },
      "id": "SrUIbzJ2PPox"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing the number of samples we'll use for each data split"
      ],
      "metadata": {
        "id": "1nK3VGMdPpzw"
      },
      "id": "1nK3VGMdPpzw"
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_samples = int(0.5 * len(raw_data))\n",
        "num_val_samples = int(0.25 * len(raw_data))\n",
        "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n",
        "print(\"num_train_samples:\", num_train_samples)\n",
        "print(\"num_val_samples:\", num_val_samples)\n",
        "print(\"num_test_samples:\", num_test_samples)"
      ],
      "metadata": {
        "id": "KyyeXQ1oPujX",
        "outputId": "d89527ed-d797-4c29-8fb9-6c8fda96f5ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KyyeXQ1oPujX",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_train_samples: 210225\n",
            "num_val_samples: 105112\n",
            "num_test_samples: 105114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing the data"
      ],
      "metadata": {
        "id": "9U4VFYT6P2Fg"
      },
      "id": "9U4VFYT6P2Fg"
    },
    {
      "cell_type": "code",
      "source": [
        "mean = raw_data[:num_train_samples].mean(axis=0)\n",
        "raw_data -= mean\n",
        "std = raw_data[:num_train_samples].std(axis=0)\n",
        "raw_data /= std"
      ],
      "metadata": {
        "id": "3ucRuBADPdtv"
      },
      "id": "3ucRuBADPdtv",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiating datasets for training, validation, and testing"
      ],
      "metadata": {
        "id": "LmI4QM8PPgsq"
      },
      "id": "LmI4QM8PPgsq"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "sampling_rate = 6\n",
        "sequence_length = 120\n",
        "delay = sampling_rate * (sequence_length + 24 - 1)\n",
        "batch_size = 256\n",
        "\n",
        "train_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=0,\n",
        "    end_index=num_train_samples)\n",
        "\n",
        "val_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples,\n",
        "    end_index=num_train_samples + num_val_samples)\n",
        "\n",
        "test_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples + num_val_samples)"
      ],
      "metadata": {
        "id": "zGdth5UeQHUg"
      },
      "id": "zGdth5UeQHUg",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing the common-sense baseline MAE"
      ],
      "metadata": {
        "id": "srULJlXtQN3Z"
      },
      "id": "srULJlXtQN3Z"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_naive_method(dataset):\n",
        "    total_abs_err = 0.\n",
        "    samples_seen = 0\n",
        "    for samples, targets in dataset:\n",
        "        preds = samples[:, -1, 1] * std[1] + mean[1]\n",
        "        total_abs_err += np.sum(np.abs(preds - targets))\n",
        "        samples_seen += samples.shape[0]\n",
        "    return total_abs_err / samples_seen\n",
        "\n",
        "print(f\"Validation MAE: {evaluate_naive_method(val_dataset):.2f}\")\n",
        "print(f\"Test MAE: {evaluate_naive_method(test_dataset):.2f}\")"
      ],
      "metadata": {
        "id": "hMIPmansQ83I",
        "outputId": "a9bdad2d-5891-4e04-aaa5-3707f8be14f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hMIPmansQ83I",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MAE: 2.44\n",
            "Test MAE: 2.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stacking recurrent layers"
      ],
      "metadata": {
        "id": "31fhaccaQ_Ir"
      },
      "id": "31fhaccaQ_Ir"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Training and evaluating a dropout-regularized, stacked GRU model**<br>\n",
        "**Experiment 1**<br>\n",
        "Replace the model-check-point callback with an early-stopping callback; set the restore_best_weights parameter to True and the patience\n",
        "parameter to 5. Monitor the validation MAE."
      ],
      "metadata": {
        "id": "ItyncZNqRhRE"
      },
      "id": "ItyncZNqRhRE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "EarlyStopping : Stop training when a monitored metric has stopped improving."
      ],
      "metadata": {
        "id": "yd8af5uyT3DX"
      },
      "id": "yd8af5uyT3DX"
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)\n",
        "x = layers.GRU(32, recurrent_dropout=0.5)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping( monitor=\"val_mae\",patience=5,restore_best_weights=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ],
      "metadata": {
        "id": "Cj6W6vK_RwJP",
        "outputId": "d42e8abb-8cbd-4e94-eb61-248f088c9c17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Cj6W6vK_RwJP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "819/819 [==============================] - 187s 225ms/step - loss: 28.2655 - mae: 3.9035 - val_loss: 9.2553 - val_mae: 2.3457\n",
            "Epoch 2/50\n",
            "819/819 [==============================] - 191s 233ms/step - loss: 14.0323 - mae: 2.9054 - val_loss: 9.2471 - val_mae: 2.3594\n",
            "Epoch 3/50\n",
            "819/819 [==============================] - 189s 231ms/step - loss: 13.2106 - mae: 2.8194 - val_loss: 9.3891 - val_mae: 2.3792\n",
            "Epoch 4/50\n",
            "819/819 [==============================] - 189s 230ms/step - loss: 12.7239 - mae: 2.7673 - val_loss: 9.0997 - val_mae: 2.3381\n",
            "Epoch 5/50\n",
            "819/819 [==============================] - 187s 229ms/step - loss: 12.1535 - mae: 2.7027 - val_loss: 8.8078 - val_mae: 2.3117\n",
            "Epoch 6/50\n",
            "819/819 [==============================] - 189s 231ms/step - loss: 11.7367 - mae: 2.6605 - val_loss: 9.4152 - val_mae: 2.3940\n",
            "Epoch 7/50\n",
            "819/819 [==============================] - 187s 229ms/step - loss: 11.4010 - mae: 2.6232 - val_loss: 8.9971 - val_mae: 2.3470\n",
            "Epoch 8/50\n",
            "819/819 [==============================] - 187s 228ms/step - loss: 11.0140 - mae: 2.5779 - val_loss: 8.8801 - val_mae: 2.3162\n",
            "Epoch 9/50\n",
            "819/819 [==============================] - 191s 233ms/step - loss: 10.6830 - mae: 2.5395 - val_loss: 8.9042 - val_mae: 2.3328\n",
            "Epoch 10/50\n",
            "819/819 [==============================] - 190s 232ms/step - loss: 10.3618 - mae: 2.5016 - val_loss: 8.7669 - val_mae: 2.3098\n",
            "Epoch 11/50\n",
            "819/819 [==============================] - 187s 229ms/step - loss: 10.1401 - mae: 2.4756 - val_loss: 8.9031 - val_mae: 2.3277\n",
            "Epoch 12/50\n",
            "819/819 [==============================] - 192s 235ms/step - loss: 9.8808 - mae: 2.4459 - val_loss: 9.3397 - val_mae: 2.3929\n",
            "Epoch 13/50\n",
            "819/819 [==============================] - 194s 236ms/step - loss: 9.6811 - mae: 2.4204 - val_loss: 10.2499 - val_mae: 2.5060\n",
            "Epoch 14/50\n",
            "819/819 [==============================] - 196s 240ms/step - loss: 9.4684 - mae: 2.3928 - val_loss: 10.0537 - val_mae: 2.4785\n",
            "Epoch 15/50\n",
            "819/819 [==============================] - 195s 238ms/step - loss: 9.3022 - mae: 2.3730 - val_loss: 10.2053 - val_mae: 2.4894\n",
            "405/405 [==============================] - 27s 66ms/step - loss: 9.7947 - mae: 2.4723\n",
            "Test MAE: 2.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Experiment 2: Adjust the number of units in each recurrent layer in the stacked setup, as well as the amount of dropout\n",
        "\n",
        "units: Positive integer, dimensionality of the output space.\n",
        "unroll: Boolean (default: False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences."
      ],
      "metadata": {
        "id": "Y-m43R3pVE0W"
      },
      "id": "Y-m43R3pVE0W"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.GRU(16, recurrent_dropout=0.3, return_sequences=True)(inputs)\n",
        "x = layers.GRU(16, recurrent_dropout=0.3)(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_stacked_gru_dropout.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "model = keras.models.load_model(\"jena_stacked_gru_dropout.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ],
      "metadata": {
        "id": "RYHwAa0_TBkh",
        "outputId": "1bb0671d-ab06-41f7-925b-b3e0c665ad03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RYHwAa0_TBkh",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "819/819 [==============================] - 159s 189ms/step - loss: 39.8764 - mae: 4.6351 - val_loss: 12.2673 - val_mae: 2.6216\n",
            "Epoch 2/50\n",
            "819/819 [==============================] - 139s 169ms/step - loss: 14.3318 - mae: 2.9289 - val_loss: 9.1999 - val_mae: 2.3433\n",
            "Epoch 3/50\n",
            "819/819 [==============================] - 139s 170ms/step - loss: 13.0486 - mae: 2.8040 - val_loss: 9.0523 - val_mae: 2.3318\n",
            "Epoch 4/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 12.5551 - mae: 2.7477 - val_loss: 8.9762 - val_mae: 2.3191\n",
            "Epoch 5/50\n",
            "819/819 [==============================] - 144s 175ms/step - loss: 12.1766 - mae: 2.7072 - val_loss: 9.0265 - val_mae: 2.3324\n",
            "Epoch 6/50\n",
            "819/819 [==============================] - 148s 181ms/step - loss: 11.8159 - mae: 2.6697 - val_loss: 8.6915 - val_mae: 2.2958\n",
            "Epoch 7/50\n",
            "819/819 [==============================] - 146s 178ms/step - loss: 11.5321 - mae: 2.6405 - val_loss: 8.9875 - val_mae: 2.3314\n",
            "Epoch 8/50\n",
            "819/819 [==============================] - 147s 179ms/step - loss: 11.2034 - mae: 2.6013 - val_loss: 9.1916 - val_mae: 2.3591\n",
            "Epoch 9/50\n",
            "819/819 [==============================] - 142s 173ms/step - loss: 10.9345 - mae: 2.5721 - val_loss: 9.0894 - val_mae: 2.3555\n",
            "Epoch 10/50\n",
            "819/819 [==============================] - 141s 172ms/step - loss: 10.7431 - mae: 2.5524 - val_loss: 8.8944 - val_mae: 2.3296\n",
            "Epoch 11/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 10.4935 - mae: 2.5241 - val_loss: 8.8584 - val_mae: 2.3235\n",
            "Epoch 12/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 10.3815 - mae: 2.5089 - val_loss: 8.8087 - val_mae: 2.3102\n",
            "Epoch 13/50\n",
            "819/819 [==============================] - 139s 170ms/step - loss: 10.2358 - mae: 2.4919 - val_loss: 9.2268 - val_mae: 2.3715\n",
            "Epoch 14/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 10.1041 - mae: 2.4778 - val_loss: 9.3117 - val_mae: 2.3895\n",
            "Epoch 15/50\n",
            "819/819 [==============================] - 138s 168ms/step - loss: 9.9811 - mae: 2.4629 - val_loss: 9.3547 - val_mae: 2.3866\n",
            "Epoch 16/50\n",
            "819/819 [==============================] - 138s 169ms/step - loss: 9.8687 - mae: 2.4486 - val_loss: 9.3020 - val_mae: 2.3847\n",
            "Epoch 17/50\n",
            "819/819 [==============================] - 138s 168ms/step - loss: 9.7777 - mae: 2.4369 - val_loss: 9.4296 - val_mae: 2.4049\n",
            "Epoch 18/50\n",
            "819/819 [==============================] - 138s 168ms/step - loss: 9.6559 - mae: 2.4212 - val_loss: 9.6074 - val_mae: 2.4192\n",
            "Epoch 19/50\n",
            "819/819 [==============================] - 139s 169ms/step - loss: 9.5908 - mae: 2.4148 - val_loss: 9.8707 - val_mae: 2.4556\n",
            "Epoch 20/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 9.4829 - mae: 2.4021 - val_loss: 9.7845 - val_mae: 2.4537\n",
            "Epoch 21/50\n",
            "819/819 [==============================] - 139s 169ms/step - loss: 9.3638 - mae: 2.3878 - val_loss: 10.1913 - val_mae: 2.4996\n",
            "Epoch 22/50\n",
            "819/819 [==============================] - 140s 170ms/step - loss: 9.3320 - mae: 2.3831 - val_loss: 10.1360 - val_mae: 2.4920\n",
            "Epoch 23/50\n",
            "819/819 [==============================] - 141s 172ms/step - loss: 9.2289 - mae: 2.3692 - val_loss: 10.4250 - val_mae: 2.5195\n",
            "Epoch 24/50\n",
            "819/819 [==============================] - 142s 173ms/step - loss: 9.2057 - mae: 2.3681 - val_loss: 10.1650 - val_mae: 2.4880\n",
            "Epoch 25/50\n",
            "819/819 [==============================] - 144s 176ms/step - loss: 9.1154 - mae: 2.3559 - val_loss: 10.4241 - val_mae: 2.5217\n",
            "Epoch 26/50\n",
            "819/819 [==============================] - 144s 176ms/step - loss: 9.0864 - mae: 2.3537 - val_loss: 10.6803 - val_mae: 2.5506\n",
            "Epoch 27/50\n",
            "819/819 [==============================] - 144s 175ms/step - loss: 8.9841 - mae: 2.3365 - val_loss: 10.2161 - val_mae: 2.4966\n",
            "Epoch 28/50\n",
            "819/819 [==============================] - 151s 184ms/step - loss: 8.9278 - mae: 2.3323 - val_loss: 10.6990 - val_mae: 2.5543\n",
            "Epoch 29/50\n",
            "819/819 [==============================] - 151s 185ms/step - loss: 8.8786 - mae: 2.3263 - val_loss: 10.5162 - val_mae: 2.5280\n",
            "Epoch 30/50\n",
            "819/819 [==============================] - 146s 177ms/step - loss: 8.8267 - mae: 2.3199 - val_loss: 10.6955 - val_mae: 2.5515\n",
            "Epoch 31/50\n",
            "819/819 [==============================] - 150s 183ms/step - loss: 8.7318 - mae: 2.3072 - val_loss: 10.7344 - val_mae: 2.5575\n",
            "Epoch 32/50\n",
            "819/819 [==============================] - 145s 177ms/step - loss: 8.6886 - mae: 2.3015 - val_loss: 10.8880 - val_mae: 2.5779\n",
            "Epoch 33/50\n",
            "819/819 [==============================] - 173s 211ms/step - loss: 8.6433 - mae: 2.2952 - val_loss: 11.0735 - val_mae: 2.5972\n",
            "Epoch 34/50\n",
            "819/819 [==============================] - 148s 180ms/step - loss: 8.5993 - mae: 2.2907 - val_loss: 11.0310 - val_mae: 2.5901\n",
            "Epoch 35/50\n",
            "819/819 [==============================] - 143s 175ms/step - loss: 8.5863 - mae: 2.2858 - val_loss: 11.0411 - val_mae: 2.5940\n",
            "Epoch 36/50\n",
            "819/819 [==============================] - 145s 177ms/step - loss: 8.5590 - mae: 2.2835 - val_loss: 11.0833 - val_mae: 2.5999\n",
            "Epoch 37/50\n",
            "819/819 [==============================] - 145s 176ms/step - loss: 8.4990 - mae: 2.2756 - val_loss: 11.1924 - val_mae: 2.6217\n",
            "Epoch 38/50\n",
            "819/819 [==============================] - 163s 199ms/step - loss: 8.4449 - mae: 2.2687 - val_loss: 11.6336 - val_mae: 2.6691\n",
            "Epoch 39/50\n",
            "819/819 [==============================] - 141s 172ms/step - loss: 8.4052 - mae: 2.2637 - val_loss: 11.2441 - val_mae: 2.6187\n",
            "Epoch 40/50\n",
            "819/819 [==============================] - 140s 170ms/step - loss: 8.4070 - mae: 2.2644 - val_loss: 11.5902 - val_mae: 2.6694\n",
            "Epoch 41/50\n",
            "819/819 [==============================] - 139s 170ms/step - loss: 8.3665 - mae: 2.2574 - val_loss: 11.4612 - val_mae: 2.6519\n",
            "Epoch 42/50\n",
            "819/819 [==============================] - 138s 169ms/step - loss: 8.3112 - mae: 2.2520 - val_loss: 11.5068 - val_mae: 2.6581\n",
            "Epoch 43/50\n",
            "819/819 [==============================] - 138s 169ms/step - loss: 8.2856 - mae: 2.2474 - val_loss: 11.4078 - val_mae: 2.6445\n",
            "Epoch 44/50\n",
            "819/819 [==============================] - 142s 173ms/step - loss: 8.2436 - mae: 2.2426 - val_loss: 11.5805 - val_mae: 2.6577\n",
            "Epoch 45/50\n",
            "819/819 [==============================] - 143s 174ms/step - loss: 8.2647 - mae: 2.2449 - val_loss: 11.3945 - val_mae: 2.6416\n",
            "Epoch 46/50\n",
            "819/819 [==============================] - 144s 176ms/step - loss: 8.2135 - mae: 2.2372 - val_loss: 11.5788 - val_mae: 2.6578\n",
            "Epoch 47/50\n",
            "819/819 [==============================] - 141s 172ms/step - loss: 8.2110 - mae: 2.2364 - val_loss: 11.9130 - val_mae: 2.7076\n",
            "Epoch 48/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 8.1554 - mae: 2.2310 - val_loss: 11.5887 - val_mae: 2.6674\n",
            "Epoch 49/50\n",
            "819/819 [==============================] - 140s 171ms/step - loss: 8.1476 - mae: 2.2295 - val_loss: 11.5778 - val_mae: 2.6613\n",
            "Epoch 50/50\n",
            "819/819 [==============================] - 139s 170ms/step - loss: 8.1245 - mae: 2.2249 - val_loss: 11.8192 - val_mae: 2.6908\n",
            "405/405 [==============================] - 21s 49ms/step - loss: 9.7268 - mae: 2.4484\n",
            "Test MAE: 2.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m7BpnG8ITnYj"
      },
      "id": "m7BpnG8ITnYj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}