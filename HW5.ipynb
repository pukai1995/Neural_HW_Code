{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3fc93d99-758c-4348-bb3e-8c777aa39728",
      "metadata": {
        "id": "3fc93d99-758c-4348-bb3e-8c777aa39728"
      },
      "source": [
        "Change our model to use a 5 × 5 kernel with kernel_size=5 passed to the nn.Conv2d constructor. <br>\n",
        "a What impact does this change have on the number of parameters in the model?<br>\n",
        "b Does the change improve or degrade overfitting?<br>\n",
        "c Read https://pytorch.org/docs/stable/nn.html#conv2d.<br>\n",
        "d Can you describe what kernel_size=(1,3) will do?<br>\n",
        "e How does the model behave with such a kernel?<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef56940c-c18c-4a06-b538-b2b0727a68d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef56940c-c18c-4a06-b538-b2b0727a68d0",
        "outputId": "c0620811-1916-4b2b-f84c-21c334dc11f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e6c98299a90>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da650d2f-60a5-4029-94b2-905866f7618a",
      "metadata": {
        "id": "da650d2f-60a5-4029-94b2-905866f7618a"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5b048c-0cb4-4b6e-8dc5-d3d0c7760423",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec5b048c-0cb4-4b6e-8dc5-d3d0c7760423",
        "outputId": "196d8932-0588-43cc-a92b-de1e0cd9c20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 60942676.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversioned/p1ch6/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "data_path = '../data-unversioned/p1ch6/'\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c34ffa-3664-475a-8f57-cca15cb0138a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5c34ffa-3664-475a-8f57-cca15cb0138a",
        "outputId": "89c26a6d-d1f1-4577-870d-ff32ffb694cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7cf5a6-df01-43eb-b317-54ce8cd6f01f",
      "metadata": {
        "id": "6d7cf5a6-df01-43eb-b317-54ce8cd6f01f"
      },
      "outputs": [],
      "source": [
        "label_map = {0: 0, 2: 1}\n",
        "class_names = ['airplane', 'bird']\n",
        "cifar2 = [(img, label_map[label])\n",
        "          for img, label in cifar10\n",
        "          if label in [0, 2]]\n",
        "cifar2_val = [(img, label_map[label])\n",
        "              for img, label in cifar10_val\n",
        "              if label in [0, 2]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c13b95-de9b-47b7-a016-f405cd0d6b53",
      "metadata": {
        "id": "71c13b95-de9b-47b7-a016-f405cd0d6b53"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula given for calculating the output size (one dimension) of a convolution is (W−F+2P)/S+1\n",
        "\n",
        "If you want to keep the output size same as the input size, you can equate (W−max(F,S)+2P)/S+1=W\n",
        "Solving this, when S=1\n",
        " gives:\n",
        "\n",
        "W−F+2P+1=W\n",
        "\n",
        "2P=F−1\n",
        "\n",
        "When S>1\n",
        ", you could solve the amount of padding needed to keep the output size same as the input size:\n",
        "\n",
        "2P=max(F,S)−S\n",
        "\n",
        "https://stats.stackexchange.com/questions/297678/how-to-calculate-optimal-zero-padding-for-convolutional-neural-networks"
      ],
      "metadata": {
        "id": "1MfTKAhylMkJ"
      },
      "id": "1MfTKAhylMkJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "089f677d-8037-4a47-81c2-d2c7a3bc1ffd",
      "metadata": {
        "id": "089f677d-8037-4a47-81c2-d2c7a3bc1ffd"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=5, padding=2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * 8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5187844-403e-4852-ab8b-23608f7ca2bb",
      "metadata": {
        "id": "f5187844-403e-4852-ab8b-23608f7ca2bb"
      },
      "source": [
        "Number of Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd1d2a48-5d7f-4ddd-98e9-8d757aa4c264",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd1d2a48-5d7f-4ddd-98e9-8d757aa4c264",
        "outputId": "5155e4cc-b5f6-4004-9ef7-52171fb9a4bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20906, [1200, 16, 3200, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model = Net()\n",
        "\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac398c6-5560-4f14-a4e0-574b1b936dce",
      "metadata": {
        "id": "fac398c6-5560-4f14-a4e0-574b1b936dce"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "497e3bda-b356-4b43-a82f-c0c622e246d1",
      "metadata": {
        "id": "497e3bda-b356-4b43-a82f-c0c622e246d1"
      },
      "outputs": [],
      "source": [
        "import datetime  # <1>\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):  # <2>\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:  # <3>\n",
        "\n",
        "            outputs = model(imgs)  # <4>\n",
        "\n",
        "            loss = loss_fn(outputs, labels)  # <5>\n",
        "\n",
        "            optimizer.zero_grad()  # <6>\n",
        "\n",
        "            loss.backward()  # <7>\n",
        "\n",
        "            optimizer.step()  # <8>\n",
        "\n",
        "            loss_train += loss.item()  # <9>\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))  # <10>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the Dataset from chapter 7; wrap it into a DataLoader; instantiate our network, an optimizer, and a loss function as before; and call our training loop.\n",
        " The substantial changes in our model from the last chapter are that now our\n",
        "model is a custom subclass of nn.Module and that we’re using convolutions. Let’s run\n",
        "training for 100 epochs while printing the loss. Depending on your hardware, this\n",
        "may take 20 minutes or more to finish!"
      ],
      "metadata": {
        "id": "l7jWG1A3c_fi"
      },
      "id": "l7jWG1A3c_fi"
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)  # <1>\n",
        "\n",
        "model = Net()  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItjS8PF5cvck",
        "outputId": "a48c2a15-60bb-4bb4-f100-6a0597b1ed18"
      },
      "id": "ItjS8PF5cvck",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-26 17:51:44.178096 Epoch 1, Training loss 0.5639230434302311\n",
            "2024-04-26 17:52:37.200346 Epoch 10, Training loss 0.3269234761880462\n",
            "2024-04-26 17:53:35.923911 Epoch 20, Training loss 0.2844390675520441\n",
            "2024-04-26 17:54:34.117760 Epoch 30, Training loss 0.2556565905072887\n",
            "2024-04-26 17:55:33.628358 Epoch 40, Training loss 0.2317938423555368\n",
            "2024-04-26 17:56:31.910825 Epoch 50, Training loss 0.2051310299118613\n",
            "2024-04-26 17:57:30.688470 Epoch 60, Training loss 0.18354635097228797\n",
            "2024-04-26 17:58:29.351167 Epoch 70, Training loss 0.1645348866464226\n",
            "2024-04-26 17:59:28.720147 Epoch 80, Training loss 0.14300961151814004\n",
            "2024-04-26 18:00:27.506357 Epoch 90, Training loss 0.12541566530515433\n",
            "2024-04-26 18:01:26.332072 Epoch 100, Training loss 0.10656613130478343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Training and Validation Loss to see the amount of overfitting"
      ],
      "metadata": {
        "id": "6SFNaP_HeBxq"
      },
      "id": "6SFNaP_HeBxq"
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # <1>\n",
        "            for imgs, labels in loader:\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # <2>\n",
        "                total += labels.shape[0]  # <3>\n",
        "                correct += int((predicted == labels).sum())  # <4>\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "UI6L8LiGmY7L",
        "outputId": "f081af52-0730-48b7-b70d-a3454f7e4bfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UI6L8LiGmY7L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.96\n",
            "Accuracy val: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernel_size can be a tuple of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimension\n",
        "\n",
        "There is a slight increase in the training accuracy but no change in Validation accuracy when we change filter size from 3x3 to 5x5"
      ],
      "metadata": {
        "id": "t-pYqzgjmb08"
      },
      "id": "t-pYqzgjmb08"
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Kernel size (1,3) using padding = 'same' which pads the input so the output has the shape as the input. However, this mode doesn’t support any stride values other than 1."
      ],
      "metadata": {
        "id": "1NqUZRivW6Gw"
      },
      "id": "1NqUZRivW6Gw"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=(1,3), padding='same')\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=(1,3), padding='same')\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * 8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "p18tNak-njXK"
      },
      "id": "p18tNak-njXK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "metadata": {
        "id": "yJ1BjjjXMw9h",
        "outputId": "c82daea4-7665-49a2-8a15-55ff6aac8a08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yJ1BjjjXMw9h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17034, [144, 16, 384, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime  # <1>\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):  # <2>\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:  # <3>\n",
        "\n",
        "            outputs = model(imgs)  # <4>\n",
        "\n",
        "            loss = loss_fn(outputs, labels)  # <5>\n",
        "\n",
        "            optimizer.zero_grad()  # <6>\n",
        "\n",
        "            loss.backward()  # <7>\n",
        "\n",
        "            optimizer.step()  # <8>\n",
        "\n",
        "            loss_train += loss.item()  # <9>\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))  # <10>"
      ],
      "metadata": {
        "id": "AziqeAiZNGwz"
      },
      "id": "AziqeAiZNGwz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)  # <1>\n",
        "\n",
        "model = Net()  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "id": "wbBiyu84NNoa",
        "outputId": "9f76590c-bbfb-4dbc-a028-86c6a0a42619",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wbBiyu84NNoa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-26 21:26:11.400002 Epoch 1, Training loss 0.5659798695023652\n",
            "2024-04-26 21:26:50.290938 Epoch 10, Training loss 0.34299551254245125\n",
            "2024-04-26 21:27:31.790345 Epoch 20, Training loss 0.3158297742817812\n",
            "2024-04-26 21:28:13.024979 Epoch 30, Training loss 0.2944501834880015\n",
            "2024-04-26 21:28:55.054121 Epoch 40, Training loss 0.27680306620658585\n",
            "2024-04-26 21:29:36.148278 Epoch 50, Training loss 0.2629637216116972\n",
            "2024-04-26 21:30:17.983434 Epoch 60, Training loss 0.24467909226941456\n",
            "2024-04-26 21:30:58.740232 Epoch 70, Training loss 0.23126557146667676\n",
            "2024-04-26 21:31:40.665242 Epoch 80, Training loss 0.21842472611149405\n",
            "2024-04-26 21:32:22.553775 Epoch 90, Training loss 0.20714929903958254\n",
            "2024-04-26 21:33:04.600887 Epoch 100, Training loss 0.19390393776973341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # <1>\n",
        "            for imgs, labels in loader:\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # <2>\n",
        "                total += labels.shape[0]  # <3>\n",
        "                correct += int((predicted == labels).sum())  # <4>\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "T2S9O6aRNRiI",
        "outputId": "f72bf4bf-fa95-44de-84a6-0f9d7e4464e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "T2S9O6aRNRiI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.92\n",
            "Accuracy val: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chap 10 Deep Learning with Python"
      ],
      "metadata": {
        "id": "WPAaLqS9sx5Q"
      },
      "id": "WPAaLqS9sx5Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 1: Load and prepare the weather data and implement the model described in\n",
        "Listing 10.23 (two GRU layers). <br>\n",
        "Replace the model-check-point callback with an early-stopping callback;  <br>\n",
        "set the restore_best_weights parameter to True and the patience parameter to 5.  <br>\n",
        "Monitor the validation MAE."
      ],
      "metadata": {
        "id": "Of-TsTC5Wzyd"
      },
      "id": "Of-TsTC5Wzyd"
    },
    {
      "cell_type": "code",
      "source": [
        "#loading and preparing the weather data\n",
        "!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
        "!unzip jena_climate_2009_2016.csv.zip"
      ],
      "metadata": {
        "id": "0jkZm524N8im",
        "outputId": "93ab8fb0-8e6f-425e-9add-7c71776702b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0jkZm524N8im",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-27 15:44:06--  https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.216.112, 52.216.134.29, 52.216.250.238, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.216.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13565642 (13M) [application/zip]\n",
            "Saving to: ‘jena_climate_2009_2016.csv.zip’\n",
            "\n",
            "jena_climate_2009_2 100%[===================>]  12.94M  41.2MB/s    in 0.3s    \n",
            "\n",
            "2024-04-27 15:44:07 (41.2 MB/s) - ‘jena_climate_2009_2016.csv.zip’ saved [13565642/13565642]\n",
            "\n",
            "Archive:  jena_climate_2009_2016.csv.zip\n",
            "  inflating: jena_climate_2009_2016.csv  \n",
            "  inflating: __MACOSX/._jena_climate_2009_2016.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting the data of the Jena weather dataset"
      ],
      "metadata": {
        "id": "0SBr8OBHOvAM"
      },
      "id": "0SBr8OBHOvAM"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "fname = os.path.join(\"jena_climate_2009_2016.csv\")\n",
        "\n",
        "with open(fname) as f:\n",
        "    data = f.read()\n",
        "\n",
        "lines = data.split(\"\\n\")\n",
        "header = lines[0].split(\",\")\n",
        "lines = lines[1:]\n",
        "print(header)\n",
        "print(len(lines))"
      ],
      "metadata": {
        "id": "q0uFxe8fOkBh",
        "outputId": "b4d7ffe7-ce89-44db-b97a-fcb99dcbc9b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "q0uFxe8fOkBh",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n",
            "420451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing the data"
      ],
      "metadata": {
        "id": "RrYUwnauO_N2"
      },
      "id": "RrYUwnauO_N2"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "temperature = np.zeros((len(lines),))\n",
        "raw_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(\",\")[1:]]\n",
        "    temperature[i] = values[1]\n",
        "    raw_data[i, :] = values[:]"
      ],
      "metadata": {
        "id": "cpINoKikPNPY"
      },
      "id": "cpINoKikPNPY",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing the data"
      ],
      "metadata": {
        "id": "SrUIbzJ2PPox"
      },
      "id": "SrUIbzJ2PPox"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing the number of samples we'll use for each data split"
      ],
      "metadata": {
        "id": "1nK3VGMdPpzw"
      },
      "id": "1nK3VGMdPpzw"
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_samples = int(0.5 * len(raw_data))\n",
        "num_val_samples = int(0.25 * len(raw_data))\n",
        "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n",
        "print(\"num_train_samples:\", num_train_samples)\n",
        "print(\"num_val_samples:\", num_val_samples)\n",
        "print(\"num_test_samples:\", num_test_samples)"
      ],
      "metadata": {
        "id": "KyyeXQ1oPujX",
        "outputId": "b9510914-dca0-41b1-9dfb-7413410fc456",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KyyeXQ1oPujX",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_train_samples: 210225\n",
            "num_val_samples: 105112\n",
            "num_test_samples: 105114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing the data"
      ],
      "metadata": {
        "id": "9U4VFYT6P2Fg"
      },
      "id": "9U4VFYT6P2Fg"
    },
    {
      "cell_type": "code",
      "source": [
        "mean = raw_data[:num_train_samples].mean(axis=0)\n",
        "raw_data -= mean\n",
        "std = raw_data[:num_train_samples].std(axis=0)\n",
        "raw_data /= std"
      ],
      "metadata": {
        "id": "3ucRuBADPdtv"
      },
      "id": "3ucRuBADPdtv",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiating datasets for training, validation, and testing"
      ],
      "metadata": {
        "id": "LmI4QM8PPgsq"
      },
      "id": "LmI4QM8PPgsq"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "sampling_rate = 6\n",
        "sequence_length = 120\n",
        "delay = sampling_rate * (sequence_length + 24 - 1)\n",
        "batch_size = 256\n",
        "\n",
        "train_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=0,\n",
        "    end_index=num_train_samples)\n",
        "\n",
        "val_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples,\n",
        "    end_index=num_train_samples + num_val_samples)\n",
        "\n",
        "test_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples + num_val_samples)"
      ],
      "metadata": {
        "id": "zGdth5UeQHUg"
      },
      "id": "zGdth5UeQHUg",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing the common-sense baseline MAE"
      ],
      "metadata": {
        "id": "srULJlXtQN3Z"
      },
      "id": "srULJlXtQN3Z"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_naive_method(dataset):\n",
        "    total_abs_err = 0.\n",
        "    samples_seen = 0\n",
        "    for samples, targets in dataset:\n",
        "        preds = samples[:, -1, 1] * std[1] + mean[1]\n",
        "        total_abs_err += np.sum(np.abs(preds - targets))\n",
        "        samples_seen += samples.shape[0]\n",
        "    return total_abs_err / samples_seen\n",
        "\n",
        "print(f\"Validation MAE: {evaluate_naive_method(val_dataset):.2f}\")\n",
        "print(f\"Test MAE: {evaluate_naive_method(test_dataset):.2f}\")"
      ],
      "metadata": {
        "id": "hMIPmansQ83I",
        "outputId": "2c035bc3-bc8e-4444-d0cf-591e480a2704",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hMIPmansQ83I",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MAE: 2.44\n",
            "Test MAE: 2.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stacking recurrent layers"
      ],
      "metadata": {
        "id": "31fhaccaQ_Ir"
      },
      "id": "31fhaccaQ_Ir"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Training and evaluating a dropout-regularized, stacked GRU model**<br>\n",
        "**Experiment 1**<br>\n",
        "Replace the model-check-point callback with an early-stopping callback; set the restore_best_weights parameter to True and the patience\n",
        "parameter to 5. Monitor the validation MAE."
      ],
      "metadata": {
        "id": "ItyncZNqRhRE"
      },
      "id": "ItyncZNqRhRE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "EarlyStopping : Stop training when a monitored metric has stopped improving."
      ],
      "metadata": {
        "id": "yd8af5uyT3DX"
      },
      "id": "yd8af5uyT3DX"
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)\n",
        "x = layers.GRU(32, recurrent_dropout=0.5)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping( monitor=\"val_mae\",patience=5,restore_best_weights=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ],
      "metadata": {
        "id": "Cj6W6vK_RwJP",
        "outputId": "d42e8abb-8cbd-4e94-eb61-248f088c9c17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Cj6W6vK_RwJP",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "819/819 [==============================] - 187s 225ms/step - loss: 28.2655 - mae: 3.9035 - val_loss: 9.2553 - val_mae: 2.3457\n",
            "Epoch 2/50\n",
            "819/819 [==============================] - 191s 233ms/step - loss: 14.0323 - mae: 2.9054 - val_loss: 9.2471 - val_mae: 2.3594\n",
            "Epoch 3/50\n",
            "819/819 [==============================] - 189s 231ms/step - loss: 13.2106 - mae: 2.8194 - val_loss: 9.3891 - val_mae: 2.3792\n",
            "Epoch 4/50\n",
            "819/819 [==============================] - 189s 230ms/step - loss: 12.7239 - mae: 2.7673 - val_loss: 9.0997 - val_mae: 2.3381\n",
            "Epoch 5/50\n",
            "819/819 [==============================] - 187s 229ms/step - loss: 12.1535 - mae: 2.7027 - val_loss: 8.8078 - val_mae: 2.3117\n",
            "Epoch 6/50\n",
            "819/819 [==============================] - 189s 231ms/step - loss: 11.7367 - mae: 2.6605 - val_loss: 9.4152 - val_mae: 2.3940\n",
            "Epoch 7/50\n",
            "819/819 [==============================] - 187s 229ms/step - loss: 11.4010 - mae: 2.6232 - val_loss: 8.9971 - val_mae: 2.3470\n",
            "Epoch 8/50\n",
            "819/819 [==============================] - 187s 228ms/step - loss: 11.0140 - mae: 2.5779 - val_loss: 8.8801 - val_mae: 2.3162\n",
            "Epoch 9/50\n",
            "819/819 [==============================] - 191s 233ms/step - loss: 10.6830 - mae: 2.5395 - val_loss: 8.9042 - val_mae: 2.3328\n",
            "Epoch 10/50\n",
            "819/819 [==============================] - 190s 232ms/step - loss: 10.3618 - mae: 2.5016 - val_loss: 8.7669 - val_mae: 2.3098\n",
            "Epoch 11/50\n",
            "819/819 [==============================] - 187s 229ms/step - loss: 10.1401 - mae: 2.4756 - val_loss: 8.9031 - val_mae: 2.3277\n",
            "Epoch 12/50\n",
            "819/819 [==============================] - 192s 235ms/step - loss: 9.8808 - mae: 2.4459 - val_loss: 9.3397 - val_mae: 2.3929\n",
            "Epoch 13/50\n",
            "819/819 [==============================] - 194s 236ms/step - loss: 9.6811 - mae: 2.4204 - val_loss: 10.2499 - val_mae: 2.5060\n",
            "Epoch 14/50\n",
            "819/819 [==============================] - 196s 240ms/step - loss: 9.4684 - mae: 2.3928 - val_loss: 10.0537 - val_mae: 2.4785\n",
            "Epoch 15/50\n",
            "819/819 [==============================] - 195s 238ms/step - loss: 9.3022 - mae: 2.3730 - val_loss: 10.2053 - val_mae: 2.4894\n",
            "405/405 [==============================] - 27s 66ms/step - loss: 9.7947 - mae: 2.4723\n",
            "Test MAE: 2.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y-m43R3pVE0W"
      },
      "id": "Y-m43R3pVE0W",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}